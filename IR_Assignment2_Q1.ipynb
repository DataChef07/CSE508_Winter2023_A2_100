{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "P0_EtILVjqmt"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#PreProcessing"
      ],
      "metadata": {
        "id": "1G1S7W2APVdf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#preprocessing\n",
        "import nltk\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "stopword = stopwords.words('english')\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "STjN3OsyPaWa",
        "outputId": "ff85f72f-44ff-4f0e-97bc-0d6b007bc378"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#this function is sued to perform preprocssing over the input query data\n",
        "\n",
        "def preprocess(sequence):\n",
        "  sequence = sequence.lower()           #convert to lowercase\n",
        "  sequence = re.sub(r'[^\\w\\s]','',sequence)     #remove punctuation\n",
        "  sequence = nltk.word_tokenize(sequence)         #perform tokenization\n",
        "  rem_stop = [s_word for s_word in sequence if s_word not in stopword]      #remove stopwords\n",
        "  sequence  = ' '.join(str(token) for token in rem_stop)\n",
        "  sequence = sequence.replace(\"  \",\" \")         #remove extra space\n",
        "\n",
        "  return sequence"
      ],
      "metadata": {
        "id": "HgmastC1PafP"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Binary"
      ],
      "metadata": {
        "id": "kQrPVOa4J6S_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def BinaryTFCalculation():\n",
        "  data_structure = []           #using 2D array as a datastructure\n",
        "  file_list=[]                  #stores all file names\n",
        "\n",
        "  arr=[]\n",
        "\n",
        "  for i in range(1, 1401):      #There are a total of 1400 files to access\n",
        "\n",
        "    fname = \"cranfield\"         #file name starts with \"cranfield\"\n",
        "\n",
        "    #Filename consists of file number. Since the number if of 4 digit we add corresponding zeros in the begining\n",
        "    if(i>=1 and i<10):\n",
        "      fname = fname + \"000\" + str(i)\n",
        "    elif(i>=10 and i<100):\n",
        "      fname = fname + \"00\" + str(i)\n",
        "    elif(i>=100 and i<1000):\n",
        "      fname = fname + \"0\" + str(i)\n",
        "    else:\n",
        "      fname = fname + str(i)\n",
        "\n",
        "    file_list.append(fname)\n",
        "\n",
        "    list_of_words ={}\n",
        "\n",
        "    with open(fname) as f:       #open the corresponding file\n",
        "      while True:\n",
        "          l = f.readline()        #read line by line\n",
        "          if not l:               #if line dosent exists means its EOF, hence get out of loop\n",
        "              break\n",
        "          else:\n",
        "            for w in l.split():   #for every word in that line\n",
        "              if w not in list_of_words:\n",
        "                list_of_words[w] = 1\n",
        "\n",
        "\n",
        "    for k,v in list_of_words.items():\n",
        "      if k not in arr:\n",
        "        arr.append(k)\n",
        "        zeroArray = []    #zeroArray contains the zeros to be appended for new words.  \n",
        "        if i!=1:\n",
        "          zeroArray = [0] * (i-1)\n",
        "        zeroArray.append(v)\n",
        "        data_structure.append(zeroArray)\n",
        "      else:\n",
        "        indexofElement = arr.index(k)\n",
        "        data_structure[indexofElement].append(v)\n",
        "\n",
        "        #And for the remaining elements(words) in the data structure we append zero for them( for that particular document)\n",
        "        for ele in data_structure:\n",
        "          if len(ele)<i-1:\n",
        "            ele.append(0)   \n",
        "\n",
        "  return data_structure, file_list, arr"
      ],
      "metadata": {
        "id": "Owo7i774Sn16"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def BinaryIDFCalculation(data_structure):\n",
        "  IDF = []\n",
        "  for i in range(0, len(data_structure)):\n",
        "    res = len(np.nonzero(data_structure[i])[0])\n",
        "    IDF.append(np.log(1400/res))\n",
        "\n",
        "  return IDF"
      ],
      "metadata": {
        "id": "ilsIfS1vSn4d"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def BinaryTFIDFCalculation(data_structure, IDF):\n",
        "  for i in range(0, len(data_structure)):\n",
        "    data_structure[i] = list(np.array(data_structure[i]) * IDF[i])\n",
        "\n",
        "  return data_structure\n"
      ],
      "metadata": {
        "id": "zGARZd6PSn8C"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Raw Count"
      ],
      "metadata": {
        "id": "8FbzPtRKJkHv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def RawCountTFCalculation():\n",
        "  data_structure = []           #using 2D array as a datastructure\n",
        "  file_list=[]                  #stores all file names ===> will be sued during OR NOT operation\n",
        "\n",
        "  arr=[]\n",
        "\n",
        "  for i in range(1, 1401):      #There are a total of 1400 files to access\n",
        "\n",
        "    fname = \"cranfield\"         #file name starts with \"cranfield\"\n",
        "\n",
        "    #Filename consists of file number. Since the number if of 4 digit we add corresponding zeros in the begining\n",
        "    if(i>=1 and i<10):\n",
        "      fname = fname + \"000\" + str(i)\n",
        "    elif(i>=10 and i<100):\n",
        "      fname = fname + \"00\" + str(i)\n",
        "    elif(i>=100 and i<1000):\n",
        "      fname = fname + \"0\" + str(i)\n",
        "    else:\n",
        "      fname = fname + str(i)\n",
        "\n",
        "    file_list.append(fname)\n",
        "\n",
        "    list_of_words ={}\n",
        "\n",
        "    with open(fname) as f:       #open the corresponding file\n",
        "      while True:\n",
        "          l = f.readline()        #read line by line\n",
        "          if not l:               #if line dosent exists means its EOF, hence get out of loop\n",
        "              break\n",
        "          else:\n",
        "            for w in l.split():   #for every word in that line\n",
        "              if w not in list_of_words:\n",
        "                list_of_words[w] = 1\n",
        "            \n",
        "              list_of_words[w] = list_of_words[w] + 1     #instead of 1 and 0 we store the count of the word.\n",
        "\n",
        "\n",
        "    for k,v in list_of_words.items():\n",
        "      if k not in arr:\n",
        "        arr.append(k)\n",
        "        zeroArray = []\n",
        "        if i!=1:\n",
        "          zeroArray = [0] * (i-1)\n",
        "        zeroArray.append(v)\n",
        "        data_structure.append(zeroArray)\n",
        "      else:\n",
        "        indexofElement = arr.index(k)\n",
        "        data_structure[indexofElement].append(v)\n",
        "        for ele in data_structure:\n",
        "          if len(ele)<i-1:\n",
        "            ele.append(0)\n",
        "\n",
        "  return data_structure, file_list, arr"
      ],
      "metadata": {
        "id": "yILgyPg0Voz4"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "p06Aa7V1LyFx"
      },
      "outputs": [],
      "source": [
        "def RawCountIDFCalculation(data_structure):\n",
        "  IDF = []\n",
        "  for i in range(0, len(data_structure)):\n",
        "    res = len(np.nonzero(data_structure[i])[0])\n",
        "    IDF.append(np.log(1400/res))\n",
        "\n",
        "  return IDF"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def RawCountTFIDFCalculation(data_structure, IDF):\n",
        "  for i in range(0, len(data_structure)):\n",
        "    data_structure[i] = list(np.array(data_structure[i]) * IDF[i])\n",
        "\n",
        "  return data_structure\n"
      ],
      "metadata": {
        "id": "4Ept4fjjbqoc"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Term frequency ===> TF Weight = f(t,d)/Pf(t‘, d)"
      ],
      "metadata": {
        "id": "-TwZbhftKA07"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def TermFrequencyTFCalculation():\n",
        "  data_structure = []           #using 2D array as a datastructure\n",
        "  file_list=[]                  #stores all file names ===> will be sued during OR NOT operation\n",
        "\n",
        "  arr=[]\n",
        "\n",
        "  for i in range(1, 1401):      #There are a total of 1400 files to access\n",
        "\n",
        "    fname = \"cranfield\"         #file name starts with \"cranfield\"\n",
        "\n",
        "    #Filename consists of file number. Since the number if of 4 digit we add corresponding zeros in the begining\n",
        "    if(i>=1 and i<10):\n",
        "      fname = fname + \"000\" + str(i)\n",
        "    elif(i>=10 and i<100):\n",
        "      fname = fname + \"00\" + str(i)\n",
        "    elif(i>=100 and i<1000):\n",
        "      fname = fname + \"0\" + str(i)\n",
        "    else:\n",
        "      fname = fname + str(i)\n",
        "\n",
        "    file_list.append(fname)\n",
        "\n",
        "    list_of_words ={}\n",
        "\n",
        "    wordCount = 0\n",
        "\n",
        "    with open(fname) as f:       #open the corresponding file\n",
        "      while True:\n",
        "          l = f.readline()        #read line by line\n",
        "          if not l:               #if line dosent exists means its EOF, hence get out of loop\n",
        "              break\n",
        "          else:\n",
        "            for w in l.split():   #for every word in that line\n",
        "              wordCount = wordCount + 1\n",
        "\n",
        "              if w not in list_of_words:\n",
        "                list_of_words[w] = 1\n",
        "            \n",
        "              list_of_words[w] = list_of_words[w] + 1 \n",
        "\n",
        "\n",
        "    for k,v in list_of_words.items():\n",
        "      list_of_words[k] = list_of_words[k]/wordCount           #take the frequency and divide by the total wordcount in the document\n",
        "\n",
        "\n",
        "    for k,v in list_of_words.items():\n",
        "      if k not in arr:\n",
        "        arr.append(k)\n",
        "        zeroArray = []\n",
        "        if i!=1:\n",
        "          zeroArray = [0] * (i-1)\n",
        "        zeroArray.append(v)\n",
        "        data_structure.append(zeroArray)\n",
        "      else:\n",
        "        indexofElement = arr.index(k)\n",
        "        data_structure[indexofElement].append(v)\n",
        "        for ele in data_structure:\n",
        "          if len(ele)<i-1:\n",
        "            ele.append(0)\n",
        "        \n",
        "\n",
        "  return data_structure, file_list, arr"
      ],
      "metadata": {
        "id": "2eM6IUARZL_R"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "6ngekziaZL_S"
      },
      "outputs": [],
      "source": [
        "def TermFrequencyIDFCalculation(data_structure):\n",
        "  IDF = []\n",
        "  for i in range(0, len(data_structure)):\n",
        "    res = len(np.nonzero(data_structure[i])[0])\n",
        "    IDF.append(np.log(1400/res))\n",
        "\n",
        "  return IDF"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def TermFrequencyTFIDFCalculation(data_structure, IDF):\n",
        "  for i in range(0, len(data_structure)):\n",
        "    data_structure[i] = list(np.array(data_structure[i]) * IDF[i])\n",
        "\n",
        "  return data_structure\n"
      ],
      "metadata": {
        "id": "45tR7NZ2ZL_S"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Log normalization ===> TF Weight = log(1+f(t,d))"
      ],
      "metadata": {
        "id": "KfTJgEupKErj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def LogNormalizationTFCalculation():\n",
        "  data_structure = []           #using 2D array as a datastructure\n",
        "  file_list=[]                  #stores all file names ===> will be sued during OR NOT operation\n",
        "\n",
        "  arr=[]\n",
        "\n",
        "  for i in range(1, 1401):      #There are a total of 1400 files to access\n",
        "\n",
        "    fname = \"cranfield\"         #file name starts with \"cranfield\"\n",
        "\n",
        "    #Filename consists of file number. Since the number if of 4 digit we add corresponding zeros in the begining\n",
        "    if(i>=1 and i<10):\n",
        "      fname = fname + \"000\" + str(i)\n",
        "    elif(i>=10 and i<100):\n",
        "      fname = fname + \"00\" + str(i)\n",
        "    elif(i>=100 and i<1000):\n",
        "      fname = fname + \"0\" + str(i)\n",
        "    else:\n",
        "      fname = fname + str(i)\n",
        "\n",
        "    file_list.append(fname)\n",
        "\n",
        "    list_of_words ={}\n",
        "\n",
        "    with open(fname) as f:       #open the corresponding file\n",
        "      while True:\n",
        "          l = f.readline()        #read line by line\n",
        "          if not l:               #if line dosent exists means its EOF, hence get out of loop\n",
        "              break\n",
        "          else:\n",
        "            for w in l.split():   #for every word in that line\n",
        "              if w not in list_of_words:\n",
        "                list_of_words[w] = 1\n",
        "            \n",
        "              list_of_words[w] = list_of_words[w] + 1 \n",
        "\n",
        "    for k,v in list_of_words.items():\n",
        "      list_of_words[k] = np.log(1 + v)      #apply log formula on the frequency obtained\n",
        "\n",
        "\n",
        "    for k,v in list_of_words.items():\n",
        "      if k not in arr:\n",
        "        arr.append(k)\n",
        "        zeroArray = []\n",
        "        if i!=1:\n",
        "          zeroArray = [0] * (i-1)\n",
        "        zeroArray.append(v)\n",
        "        data_structure.append(zeroArray)\n",
        "      else:\n",
        "        indexofElement = arr.index(k)\n",
        "        data_structure[indexofElement].append(v)\n",
        "        for ele in data_structure:\n",
        "          if len(ele)<i-1:\n",
        "            ele.append(0)\n",
        "\n",
        "  return data_structure, file_list, arr"
      ],
      "metadata": {
        "id": "ryzFrsnGWY4i"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def LogNormalizationIDFCalculation(data_structure):\n",
        "  IDF = []\n",
        "  for i in range(0, len(data_structure)):\n",
        "    res = len(np.nonzero(data_structure[i])[0])\n",
        "    IDF.append(np.log(1400/res))\n",
        "\n",
        "  return IDF"
      ],
      "metadata": {
        "id": "qbYqsPk7M8cK"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def LogNormalizationTFTDFCalculation(data_structure, IDF):\n",
        "  for i in range(0, len(data_structure)):\n",
        "    data_structure[i] = list(np.array(data_structure[i]) * IDF[i])\n",
        "\n",
        "  return data_structure"
      ],
      "metadata": {
        "id": "YpMN-YoaWLLw"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Double normalization ===> TF Weight = 0.5+0.5*(f(t,d)/ max(f(t‘,d))"
      ],
      "metadata": {
        "id": "XlKmsh0jKEyx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def DoubleNormalizationTFCalculation():\n",
        "  data_structure = []           #using 2D array as a datastructure\n",
        "  file_list=[]                  #stores all file names ===> will be sued during OR NOT operation\n",
        "\n",
        "  arr=[]\n",
        "\n",
        "  for i in range(1, 1401):      #There are a total of 1400 files to access\n",
        "\n",
        "    fname = \"cranfield\"         #file name starts with \"cranfield\"\n",
        "\n",
        "    #Filename consists of file number. Since the number if of 4 digit we add corresponding zeros in the begining\n",
        "    if(i>=1 and i<10):\n",
        "      fname = fname + \"000\" + str(i)\n",
        "    elif(i>=10 and i<100):\n",
        "      fname = fname + \"00\" + str(i)\n",
        "    elif(i>=100 and i<1000):\n",
        "      fname = fname + \"0\" + str(i)\n",
        "    else:\n",
        "      fname = fname + str(i)\n",
        "\n",
        "    file_list.append(fname)\n",
        "\n",
        "    list_of_words ={}\n",
        "\n",
        "    with open(fname) as f:       #open the corresponding file\n",
        "      while True:\n",
        "          l = f.readline()        #read line by line\n",
        "          if not l:               #if line dosent exists means its EOF, hence get out of loop\n",
        "              break\n",
        "          else:\n",
        "            for w in l.split():   #for every word in that line\n",
        "              if w not in list_of_words:\n",
        "                list_of_words[w] = 1\n",
        "            \n",
        "              list_of_words[w] = list_of_words[w] + 1 \n",
        "\n",
        "\n",
        "    for k,v in list_of_words.items():\n",
        "      if k not in arr:\n",
        "        arr.append(k)\n",
        "        zeroArray = []\n",
        "        if i!=1:\n",
        "          zeroArray = [0] * (i-1)\n",
        "        zeroArray.append(v)\n",
        "        data_structure.append(zeroArray)\n",
        "      else:\n",
        "        indexofElement = arr.index(k)\n",
        "        data_structure[indexofElement].append(v)\n",
        "        for ele in data_structure:\n",
        "          if len(ele)<i-1:\n",
        "            ele.append(0)\n",
        "\n",
        "  for i in range(0, len(data_structure)):\n",
        "    max_ele = float(max(data_structure[i]))\n",
        "    data_structure[i] = list((np.array(data_structure[i]) / max_ele)* 0.5 + 0.5)\n",
        "\n",
        "  return data_structure, file_list, arr"
      ],
      "metadata": {
        "id": "EiT3GelKYJSw"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def DoubleNormalizationIDFCalculation(data_structure):\n",
        "  IDF = []\n",
        "  for i in range(0, len(data_structure)):\n",
        "    res = len(np.nonzero(data_structure[i])[0])\n",
        "    IDF.append(np.log(1400/res))\n",
        "\n",
        "  return IDF"
      ],
      "metadata": {
        "id": "kR4vNKI3YJSx"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def DoubleNormalizationTFIDFCalculation(data_structure, IDF):\n",
        "  for i in range(0, len(data_structure)):\n",
        "    data_structure[i] = list(np.array(data_structure[i]) * IDF[i])\n",
        "\n",
        "  return data_structure"
      ],
      "metadata": {
        "id": "N3sjMiK7YJSy"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Convert to a dataframe ( Matrix )"
      ],
      "metadata": {
        "id": "k2GKsQcIxJpt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def convertToDataframe(data_structure, file_list, arr):\n",
        "  df = pd.DataFrame(data_structure, columns =file_list) \n",
        "  df['cranfield1400'] = df['cranfield1400'].fillna(0)\n",
        "  return df"
      ],
      "metadata": {
        "id": "exYpYw6pxPFO"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def dataframeWithWords(data_structure, file_list, arr):\n",
        "  df = pd.DataFrame(data_structure, columns =file_list) \n",
        "  df.insert(loc = 0, column = 'words', value = arr)\n",
        "  df['cranfield1400'] = df['cranfield1400'].fillna(0)\n",
        "  return df"
      ],
      "metadata": {
        "id": "jLr59i3GSs0W"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Calculate TF-IDF Matrices for all the different weights"
      ],
      "metadata": {
        "id": "fdW_M6RndycL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------- Binary ----------------------------------\n",
        "data_structure_Binary, file_list, arr = BinaryTFCalculation()\n",
        "IDF_Binary = BinaryIDFCalculation(data_structure_Binary)\n",
        "TFIDF_Binary = BinaryTFIDFCalculation(data_structure_Binary, IDF_Binary)\n",
        "TFIDF_Matrix_Binary = convertToDataframe(data_structure_Binary, file_list, arr)\n",
        "TFIDF_WordMatrix_Binary = dataframeWithWords(data_structure_Binary, file_list, arr)"
      ],
      "metadata": {
        "id": "IY1lu8_udxDR"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------- Raw Count ----------------------------------\n",
        "data_structure_RawCount, file_list, arr = RawCountTFCalculation()\n",
        "IDF_RawCount = RawCountIDFCalculation(data_structure_RawCount)\n",
        "TFIDF_RawCount = RawCountTFIDFCalculation(data_structure_RawCount, IDF_RawCount)\n",
        "TFIDF_Matrix_RawCount = convertToDataframe(data_structure_RawCount, file_list, arr)\n",
        "TFIDF_WordMatrix_RawCount = dataframeWithWords(data_structure_RawCount, file_list, arr)"
      ],
      "metadata": {
        "id": "aBqRyXXd2_nz"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------- Term Frequency ----------------------------------\n",
        "data_structure_TermFrequency, file_list, arr = TermFrequencyTFCalculation()\n",
        "IDF_TermFrequency = TermFrequencyIDFCalculation(data_structure_TermFrequency)\n",
        "TFIDF_TermFrequency = TermFrequencyTFIDFCalculation(data_structure_TermFrequency, IDF_TermFrequency)\n",
        "TFIDF_Matrix_TermFrequency = convertToDataframe(data_structure_TermFrequency, file_list, arr)\n",
        "TFIDF_WordMatrix_TermFrequency = dataframeWithWords(data_structure_TermFrequency, file_list, arr)"
      ],
      "metadata": {
        "id": "Qns9dsrO2_rx"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------- Log Normalization ----------------------------------\n",
        "data_structure_LogNormalization, file_list, arr = LogNormalizationTFCalculation()\n",
        "IDF_LogNormalization = LogNormalizationIDFCalculation(data_structure_LogNormalization)\n",
        "TFIDF_LogNormalization = LogNormalizationTFTDFCalculation(data_structure_LogNormalization, IDF_LogNormalization)\n",
        "TFIDF_Matrix_LogNormalization = convertToDataframe(data_structure_LogNormalization, file_list, arr)\n",
        "TFIDF_WordMatrix_LogNormalization = dataframeWithWords(data_structure_LogNormalization, file_list, arr)"
      ],
      "metadata": {
        "id": "OmCuZoRt2_wD"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------- Double Normalization ----------------------------------\n",
        "data_structure_DoubleNormalization, file_list, arr = DoubleNormalizationTFCalculation()\n",
        "IDF_DoubleNormalization = DoubleNormalizationIDFCalculation(data_structure_DoubleNormalization)\n",
        "TFIDF_DoubleNormalization = DoubleNormalizationTFIDFCalculation(data_structure_DoubleNormalization, IDF_DoubleNormalization)\n",
        "TFIDF_Matrix_DoubleNormalization = convertToDataframe(data_structure_DoubleNormalization, file_list, arr)\n",
        "TFIDF_WordMatrix_DoubleNormalization = dataframeWithWords(data_structure_DoubleNormalization, file_list, arr)"
      ],
      "metadata": {
        "id": "KkdBU0tj3H9T"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Jaccard Coefficient\n"
      ],
      "metadata": {
        "id": "8lbIla4Cd2l9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def JaccardCoefficient(query):\n",
        "\n",
        "  similarityArray = {}\n",
        "\n",
        "  data_structure_for_Jaccard = list(TFIDF_Matrix_Binary.to_numpy())\n",
        "\n",
        "  for i in range(0, 1400):\n",
        "\n",
        "    intersection = 0      #keeps track of intersection\n",
        "    DocSet = 0  #total words\n",
        "\n",
        "    wordList = query.split()        #contains all the words in the query after preprocessing\n",
        "    wordSet = len(wordList)         #total words in query\n",
        "\n",
        "    for j in wordList:\n",
        "      if j in arr:\n",
        "        indexofElement = arr.index(j)\n",
        "        val = data_structure_for_Jaccard[indexofElement][i]\n",
        "        if val!=0:\n",
        "          intersection = intersection + 1\n",
        "\n",
        "    for  j in range(0, len(data_structure_for_Jaccard)):\n",
        "      val = data_structure_for_Jaccard[j][i]\n",
        "      if val!=0:\n",
        "        DocSet = DocSet + 1\n",
        "\n",
        "    union = wordSet + DocSet - intersection   #compute union\n",
        "\n",
        "    JaccardValue = intersection / union\n",
        "\n",
        "    similarityArray[file_list[i]] = JaccardValue\n",
        "\n",
        "\n",
        "  similarityArray = dict(sorted(similarityArray.items(), key=lambda item: item[1], reverse = True))\n",
        "  topFive = list(similarityArray)[0:5]\n",
        "  finalList ={}\n",
        "  for ele in topFive:\n",
        "    finalList[ele] = similarityArray[ele]\n",
        "  return finalList\n"
      ],
      "metadata": {
        "id": "0p-6a-uMdwJg"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def topfive(similarityArray, scheme):\n",
        "  similarityArray = dict(sorted(similarityArray.items(), key=lambda item: item[1], reverse = True))\n",
        "  topFive = list(similarityArray)[0:5]\n",
        "  finalList ={}\n",
        "  for ele in topFive:\n",
        "    finalList[ele] = similarityArray[ele]\n",
        "\n",
        "  print()\n",
        "  print('------------------------------------------- Weighting Scheme:  ' + scheme + ' -------------------------------------------')\n",
        "  print(finalList)\n",
        "  print()\n",
        "  print()\n",
        "\n",
        "  \n"
      ],
      "metadata": {
        "id": "QPfOpzq7dwLo"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Driver Code ===> Jaccard\n"
      ],
      "metadata": {
        "id": "Mf8q5ZNREtVm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = input(\"Enter your query   ===>  \")\n",
        "if query ==\"\":\n",
        "  print(\"cannot perform any operation on empty query. Please provide some input\")\n",
        "else:\n",
        "  query = preprocess(query)\n",
        "  JaccardList = JaccardCoefficient(query)\n",
        "  print(JaccardList)\n",
        "\n",
        "#takeoff conformal circulation tapered spaced table incidences\n",
        "# ['cranfield1358', 'cranfield1162', 'cranfield0246', 'cranfield1357', 'cranfield0249']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4pv3o1dDdwPp",
        "outputId": "4262096c-c515-466e-bc65-4c50c6a5fe76"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your query   ===>  takeoff conformal circulation tapered spaced table incidences of or the\n",
            "{'cranfield1358': 0.04, 'cranfield1162': 0.034482758620689655, 'cranfield0246': 0.03278688524590164, 'cranfield1357': 0.03225806451612903, 'cranfield0249': 0.031746031746031744}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Driver Code ===> TFIDF"
      ],
      "metadata": {
        "id": "nXLOhDLjWGrG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy.linalg import norm\n",
        "from sklearn.metrics.pairwise import cosine_similarity"
      ],
      "metadata": {
        "id": "4k-tRm5Rht00"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = input(\"Enter your query   ===>  \")\n",
        "if query ==\"\":\n",
        "  print(\"cannot perform any operation on empty query. Please provide some input\")\n",
        "else:\n",
        "  query = preprocess(query)\n",
        "\n",
        "  #Convert the TFIDF matrices obtained for all the weighting schemes into a numpy array for easy calculation \n",
        "  #and store them into different arrays respectively.\n",
        "\n",
        "  binary = list(TFIDF_Matrix_Binary.to_numpy())\n",
        "  raw = list(TFIDF_Matrix_RawCount.to_numpy())\n",
        "  term_frequency = list(TFIDF_Matrix_TermFrequency.to_numpy())\n",
        "  log_norm = list(TFIDF_Matrix_LogNormalization.to_numpy())\n",
        "  double_norm = list(TFIDF_Matrix_DoubleNormalization.to_numpy()) \n",
        "  \n",
        "\n",
        "  #Score Dictionaries ( eg. score_Binary, score_raw, …etc) are used to keep track of the TFIDF scores of the query for every document.  \n",
        "\n",
        "  score_Binary ={}\n",
        "  score_raw ={}\n",
        "  score_term_frequency ={}\n",
        "  score_log_norm ={}\n",
        "  score_double_norm ={}\n",
        "\n",
        "  wordList = query.split()\n",
        "\n",
        "  for i in range(0, 1400):\n",
        "\n",
        "    #The val list ( eg. val_Binary, val_raw ….. etc) holds the TFIDF values for the word from the TFIDF matrices.\n",
        "\n",
        "\n",
        "    val_Binary =[]\n",
        "    val_raw =[]\n",
        "    val_term_frequency =[]\n",
        "    val_log_norm =[]\n",
        "    val_double_norm =[]\n",
        "\n",
        "    for j in wordList:\n",
        "\n",
        "      if j in arr:\n",
        "        indexofElement = arr.index(j)\n",
        "        val_Binary.append(binary[indexofElement][i])\n",
        "        val_raw.append(raw[indexofElement][i])\n",
        "        val_term_frequency.append(term_frequency[indexofElement][i])\n",
        "        val_log_norm.append(log_norm[indexofElement][i])\n",
        "        val_double_norm.append(double_norm[indexofElement][i])\n",
        "\n",
        "    temp_binary = 0\n",
        "    temp_raw = 0\n",
        "    temp_term_frequency = 0\n",
        "    term_log_norm = 0\n",
        "    term_double_norm = 0\n",
        "    for count in range(0, len(val_Binary)):\n",
        "      temp_binary = temp_binary + val_Binary[count]\n",
        "      temp_raw = temp_raw + val_raw[count]\n",
        "      temp_term_frequency = temp_term_frequency + val_term_frequency[count]\n",
        "      term_log_norm = term_log_norm + val_log_norm[count]\n",
        "      term_double_norm = term_double_norm + val_double_norm[count]\n",
        "\n",
        "    #These values are then used to obtain the overall score for the query ( for this we have used some temporary variables called temp_Binary, … etc.). \n",
        "    #Post obtaining the scores for the document add them to the score dictionaries with the key being the document name \n",
        "    #and the value being the score of the query for that document.\n",
        "    score_Binary[file_list[i]] = temp_binary\n",
        "    score_raw[file_list[i]] = temp_raw\n",
        "    score_term_frequency[file_list[i]] = temp_term_frequency\n",
        "    score_log_norm[file_list[i]] = term_log_norm\n",
        "    score_double_norm[file_list[i]] = term_double_norm\n",
        "\n",
        "  topfive(score_Binary, 'Binary')\n",
        "  topfive(score_raw, 'Raw Count')\n",
        "  topfive(score_term_frequency, 'Term Frequency')\n",
        "  topfive(score_log_norm, 'Log Normalization')\n",
        "  topfive(score_double_norm, 'Double Normalization')    \n",
        "\n"
      ],
      "metadata": {
        "id": "xH8gH3A-WJh6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f27fa49-b4a2-42d2-8733-95306e443895"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your query   ===>  takeoff conformal circulation tapered spaced table incidences\n",
            "\n",
            "------------------------------------------- Weighting Scheme:  Binary -------------------------------------------\n",
            "{'cranfield0246': 10.298800289180274, 'cranfield0452': 10.298800289180274, 'cranfield0249': 9.924106839738865, 'cranfield1162': 9.89333518107211, 'cranfield0315': 9.692664485609958}\n",
            "\n",
            "\n",
            "\n",
            "------------------------------------------- Weighting Scheme:  Raw Count -------------------------------------------\n",
            "{'cranfield0246': 26.050068624735843, 'cranfield1162': 24.83367330041135, 'cranfield0793': 24.231661214024896, 'cranfield0452': 20.59760057836055, 'cranfield1095': 20.188011753068523}\n",
            "\n",
            "\n",
            "\n",
            "------------------------------------------- Weighting Scheme:  Term Frequency -------------------------------------------\n",
            "{'cranfield0778': 0.4199400763954721, 'cranfield1358': 0.41318287791388114, 'cranfield1357': 0.35619213613265616, 'cranfield0246': 0.3339752387786647, 'cranfield1162': 0.33111564400548466}\n",
            "\n",
            "\n",
            "\n",
            "------------------------------------------- Weighting Scheme:  Log Normalization -------------------------------------------\n",
            "{'cranfield0246': 12.882965863790488, 'cranfield1162': 12.320871870789274, 'cranfield0793': 12.042683217228086, 'cranfield0452': 11.314388556232132, 'cranfield0249': 10.902745728192356}\n",
            "\n",
            "\n",
            "\n",
            "------------------------------------------- Weighting Scheme:  Double Normalization -------------------------------------------\n",
            "{'cranfield0793': 0.002679528517008189, 'cranfield0246': 0.0026795285170081885, 'cranfield1162': 0.0026497559779303202, 'cranfield0315': 0.0025604383606967138, 'cranfield0452': 0.0025604383606967138}\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "References:\n",
        "\n",
        "https://www.geeksforgeeks.org/python-count-occurrences-of-each-word-in-given-text-file/\n",
        "\n",
        "https://www.geeksforgeeks.org/numpy-nonzero-in-python/\n",
        "\n",
        "https://www.geeksforgeeks.org/python-index-of-non-zero-elements-in-python-list/\n",
        "\n",
        "https://www.geeksforgeeks.org/python-constant-multiplication-over-list/\n",
        "\n",
        "https://stackoverflow.com/questions/20577840/python-dictionary-sorting-in-descending-order-based-on-values\n",
        "\n"
      ],
      "metadata": {
        "id": "U6DNJajpYKJ4"
      }
    }
  ]
}